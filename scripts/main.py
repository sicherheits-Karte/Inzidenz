import requests
from bs4 import BeautifulSoup
import time
import re
import os
from datetime import datetime
import sys

# Debug: Aktuelles Verzeichnis anzeigen
print("üìÅ Aktuelles Arbeitsverzeichnis:", os.getcwd())
print("üìÅ Dateien im aktuellen Verzeichnis:", os.listdir("."))

# Pfad zur Koordinaten-Datei (im gleichen Verzeichnis)
try:
    sys.path.append(os.path.dirname(__file__))
    from koordinaten import CITY_COORDINATES
    print("‚úÖ Koordinaten erfolgreich geladen")
    print(f"üìä Geladene St√§dte: {len(CITY_COORDINATES)}")
except ImportError as e:
    print(f"‚ùå Fehler beim Koordinaten-Import: {e}")
    CITY_COORDINATES = {}

# URL der Blaulicht-Seite
URL = "https://www.presseportal.de/blaulicht/d/polizei"
# Schlagw√∂rter
KEYWORDS = ["Einbruch", "Diebstahl", "Unfall"]
# Ausgabedatei im data/ Ordner
OUTPUT_FILE = "../data/einbrueche_diebstaehle.txt"
# Header
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
}
# Datum-Regex
DATE_PATTERN = re.compile(
    r"\b(\d{1,2})\.(\d{1,2})\.(\d{4})"
    r"|\b(\d{1,2})\.\s*(Januar|Februar|M√§rz|April|Mai|Juni|Juli|August|September|Oktober|November|Dezember)\s*(\d{4})\b",
    re.IGNORECASE
)
# Ausschlussw√∂rter - ERWEITERT wie im alten Code
EXCLUDE_WORDS = {
    "Container", "Fahrschule", "Montag", "Dienstag", "Mittwoch", "Donnerstag", "Freitag", "Samstag", "Sonntag",
    "Polizei", "Ermittler", "T√§ter", "Zeugen", "Hinweise", "Opfer", "Einbrecher", "Diebe", "Fahrzeug", "Wohnung",
    "Haus", "Gesch√§ft", "Firma", "Unternehmen", "Ladendiebstahl", "PKW", "LKW", "Auto", "Werkstatt", "Garage",
    "Idstein", "Black", "Deckerstra√üe", "Diebe", "T√§ter"
}

# --- Global: Bereits vorhandene Eintr√§ge (zur Duplikatvermeidung) ---
existing_entries = set()

def load_existing_entries():
    """Lade alle bestehenden Eintr√§ge aus der TXT-Datei in ein Set (ohne Kommentare mit // am Anfang)"""
    global existing_entries
    existing_entries = set()
    if not os.path.exists(OUTPUT_FILE):
        print("üìù Output-Datei existiert noch nicht")
        return
    try:
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line.startswith("//"):
                    continue  # √úberspringe Kommentare und Header
                if line:
                    # Normalisiere: nur der eigentliche Eintrag (JSON + // Kommentar)
                    existing_entries.add(line)
        print(f"üìä {len(existing_entries)} vorhandene Eintr√§ge geladen")
    except Exception as e:
        print(f"‚ùå Fehler beim Laden vorhandener Eintr√§ge: {e}")

def is_duplicate(entry_line):
    """Pr√ºft, ob der Eintrag bereits in der Datei existiert"""
    return entry_line in existing_entries

def extract_date_from_text(text):
    match = DATE_PATTERN.search(text)
    if not match:
        return None
    if match.group(1): # DD.MM.YYYY
        day, month, year = match.group(1), match.group(2), match.group(3)
        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    if match.group(4): # 1. November 2025
        day, month_name, year = match.group(4), match.group(5), match.group(6)
        months = {
            "Januar": "01", "Februar": "02", "M√§rz": "03", "April": "04", "Mai": "05", "Juni": "06",
            "Juli": "07", "August": "08", "September": "09", "Oktober": "10", "November": "11", "Dezember": "12"
        }
        month = months.get(month_name.capitalize(), "01")
        return f"{year}-{month}-{day.zfill(2)}"
    return None

def extract_location_from_article(article):
    full_text = article.get_text(separator=" ", strip=True)
   
    # 1. (ots) Ort - VERBESSERTE REGEX wie im alten Code
    ots_match = re.search(r"([A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü\-]+(?:\s+[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü\-]+)*)\s*\(ots\)", full_text)
    if ots_match:
        loc = ots_match.group(1).strip()
        if loc not in EXCLUDE_WORDS:
            return loc
    
    # 2. "in [Ort]", "bei [Ort]" etc. - VERBESSERTE LOGIK
    for prefix in ["in", "bei", "aus", "von", "im", "am", "an"]:
        pattern = rf"{prefix}\s+([A-Z√Ñ√ñ√ú][A-Za-z√§√∂√º√ü0-9\-\.,]+(?:\s+[A-Z√Ñ√ñ√ú][A-Za-z√§√∂√º√ü0-9\-\.,]+)*)"
        matches = re.finditer(pattern, full_text, re.IGNORECASE)
        for m in matches:
            candidate = m.group(1).strip(".,;")
            if (len(candidate) >= 3 and
                candidate not in EXCLUDE_WORDS and
                not any(day in candidate.lower() for day in ["montag", "dienstag", "mittwoch", "donnerstag", "freitag", "samstag", "sonntag"]) and
                any(c.isupper() for c in candidate)):
                return candidate
    return None

def save_to_file(location, keyword, date_str):
    if not date_str:
        return

    coords = []
    used_city = None

    if location:
        # VERBESSERTE ORTSEXTRAKTION wie im alten Code
        words = re.findall(r'[A-Z√Ñ√ñ√ú][a-z√§√∂√º√ü]+', location)
        for word in words:
            if word in CITY_COORDINATES:
                coords = CITY_COORDINATES[word][::-1]
                used_city = word
                break

    # Baue die Ausgabezeile - GENAU WIE IM ALTEN CODE
    if not location:
        line = f'//{{"coords": [], "date": "{date_str}"}} // Kein Ort gefunden, {keyword}'
        status = "Ort nicht erkannt"
    elif coords:
        line = f'{{"coords": {coords}, "date": "{date_str}"}}, // {location}, {keyword}'
        status = f"Koordinaten f√ºr '{used_city}'"
    else:
        line = f'//{{"coords": [], "date": "{date_str}"}} // {location}, {keyword}'
        status = "Koordinaten fehlen"

    # DUPILKATPR√úFUNG
    if is_duplicate(line):
        print(f"‚ö†Ô∏è Doppelt (√ºbersprungen): {location or '‚Äî'} ({keyword}) am {date_str}")
        return

    # Schreibe nur neue Eintr√§ge
    try:
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(line + "\n")
        existing_entries.add(line)  # Merke f√ºr zuk√ºnftige Pr√ºfungen
        print(f"‚úÖ {status}: {location or '‚Äî'} ({keyword}) am {date_str}")
    except Exception as e:
        print(f"‚ùå Fehler beim Schreiben: {e}")

def check_website():
    try:
        print("üåê Starte Web-Scraping...")
        response = requests.get(URL, headers=HEADERS, timeout=15)
        response.raise_for_status()
        print("‚úÖ Website erfolgreich geladen")
        
        soup = BeautifulSoup(response.text, "html.parser")
        articles = soup.find_all("article") or soup.find_all("div", class_=re.compile(r"teaser|news|item", re.I))
        print(f"üì∞ Gefundene Artikel: {len(articles)}")
        
        found_count = 0
        for article in articles:
            text_block = article.get_text(separator=" ", strip=True).lower()
            if not any(kw.lower() in text_block for kw in KEYWORDS):
                continue
                
            location = extract_location_from_article(article)
            date_str = extract_date_from_text(article.get_text())
            
            if not date_str:
                continue
                
            keyword = next((kw for kw in KEYWORDS if kw.lower() in text_block), "Unbekannt")
            print(f"üìç Gefunden: {location} | {keyword} | {date_str}")
            save_to_file(location, keyword, date_str)
            found_count += 1
            
        print(f"üìä Gesamt gefunden: {found_count} relevante Eintr√§ge")
            
    except Exception as e:
        print(f"‚ùå Fehler beim Scraping: {e}")

# --- Hauptprogramm ---
if __name__ == "__main__":
    # Stelle sicher, dass der Ausgabeordner existiert
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    # Initialisiere Ausgabedatei mit Header (falls neu) - WIE IM ALTEN CODE
    if not os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(f"// √úberwachung gestartet: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"// Schlagw√∂rter: {', '.join(KEYWORDS)}\n")
        print("üìÑ Neue Ausgabedatei erstellt")
    else:
        # Lade bestehende Eintr√§ge, um Duplikate zu vermeiden
        load_existing_entries()

    print("üöÄ Starte Blaulicht-Monitor...")
    print(f"‚Üí {len(existing_entries)} Eintr√§ge bereits vorhanden.")
    
    # EINMALIGES SCRAPING f√ºr GitHub Actions (keine Endlosschleife)
    check_website()
    print("‚úÖ Scraping abgeschlossen")
